{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "14ea6e2d-1e6a-46ab-b7fd-fbb84fa2fb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import Counter\n",
    "import warnings\n",
    "\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ec14b66a-09b1-4ead-9f11-ec3c36391e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeableNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\lib\\site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy in c:\\programdata\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\programdata\\anaconda3\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: nltk in c:\\programdata\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\programdata\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.15.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (8.1.8)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy scikit-learn nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8e1806b8-0ac9-484b-b87a-88082dc63e8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Downloading NLTK resources (This runs automatically)...\n",
      "NLTK downloads complete.\n"
     ]
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "# These downloads are required for tokenization, stopwords, and lemmatization\n",
    "print(\"Step 0: Downloading NLTK resources (This runs automatically)...\")\n",
    "try:\n",
    "    nltk.download('punkt', quiet=True)\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    nltk.download('wordnet', quiet=True)\n",
    "    print(\"NLTK downloads complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error during NLTK download: {e}. Please ensure you have an internet connection.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa89b5fd-ad94-43bc-aaee-4c633d57b6dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1: Loading Dataset ('data.csv')\n",
      "Failed to load with encoding: 'utf-8'. Trying next...\n",
      "Successfully loaded data from 'data.csv' using encoding: 'latin-1'.\n",
      "Dataset loaded with 540455 rows. Using column 'Description' for NLP.\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Data Loading ---\n",
    "print(\"\\nStep 1: Loading Dataset ('data.csv')\")\n",
    "def load_data_with_encoding(file_path):\n",
    "    \"\"\"Attempts to load a CSV file using multiple common encodings to handle UnicodeDecodeError.\"\"\"\n",
    "    encodings_to_try = ['utf-8', 'latin-1', 'cp1252']\n",
    "    for encoding in encodings_to_try:\n",
    "        try:\n",
    "            # Attempt to load the user's specified CSV file with the current encoding\n",
    "            df = pd.read_csv(file_path, encoding=encoding)\n",
    "            print(f\"Successfully loaded data from '{file_path}' using encoding: '{encoding}'.\")\n",
    "            return df\n",
    "        except UnicodeDecodeError:\n",
    "            print(f\"Failed to load with encoding: '{encoding}'. Trying next...\")\n",
    "        except FileNotFoundError:\n",
    "            # Re-raise FileNotFoundError if the file itself isn't found\n",
    "            raise FileNotFoundError(f\"The file '{file_path}' was not found.\")\n",
    "    \n",
    "    # If all attempts fail, return simulated data\n",
    "    print(\"WARNING: All common encoding attempts failed or 'data.csv' not found. Using simulated data for demonstration.\")\n",
    "    data = {\n",
    "        'InvoiceNo': [536365, 536365, 536366, 536367, 536367, 536368],\n",
    "        'StockCode': ['85123A', '71053', '22632', '22745', '22748', '22310'],\n",
    "        'Description': [\n",
    "            'WHITE HANGING HEART T-LIGHT HOLDER',\n",
    "            'WHITE METAL LANTERN',\n",
    "            'HAND WARMER RED POLKA DOT',\n",
    "            \"POPPY'S PATCHWORK BAG\",\n",
    "            \"POPPY'S PATCHWORK KIT\",\n",
    "            'IVORY KNITTED MUG COVER'\n",
    "        ],\n",
    "        'Quantity': [6, 6, 6, 6, 6, 6],\n",
    "        'UnitPrice': [2.55, 3.39, 1.85, 2.10, 2.10, 1.65],\n",
    "        'CustomerID': [17850, 17850, 17850, 13047, 13047, 13047],\n",
    "        'Country': ['United Kingdom'] * 6\n",
    "    }\n",
    "    return pd.DataFrame(data)\n",
    "# Call the robust loading function\n",
    "try:\n",
    "    df = load_data_with_encoding('data.csv')\n",
    "except FileNotFoundError as e:\n",
    "    print(e)\n",
    "    # Load simulated data if file is not found (and loading function didn't already handle it)\n",
    "    df = load_data_with_encoding(None) \n",
    "\n",
    "\n",
    "# Ensure the 'Description' column is used for NLP operations\n",
    "TEXT_COLUMN = 'Description'\n",
    "# Drop rows where Description is missing, as we can't process them\n",
    "df.dropna(subset=[TEXT_COLUMN], inplace=True)\n",
    "\n",
    "print(f\"Dataset loaded with {df.shape[0]} rows. Using column '{TEXT_COLUMN}' for NLP.\")\n",
    "print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38806d89-aa7f-44b7-abb7-98b458269ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32e3294b-54f9-4198-8a3d-aad989de35eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# I. CLASS TASK: TOKENIZATION AND WORD EMBEDDINGS\n",
    "# ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b6a1e047-7b96-4503-9113-fedf91414712",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2: Tokenization (Breaking text into individual words/tokens)\n",
      "Original Text Sample:\n",
      "'WHITE HANGING HEART T-LIGHT HOLDER'\n",
      "\n",
      "Resulting Tokens:\n",
      "['white', 'hanging', 'heart', 't-light', 'holder']\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Step 2: Tokenization ---\n",
    "print(\"\\nStep 2: Tokenization (Breaking text into individual words/tokens)\")\n",
    "# Tokenization is the fundamental NLP process of splitting a sequence of text into smaller units \n",
    "# called tokens (usually words or punctuation). This converts continuous text into a list \n",
    "# of discrete items for machine processing.\n",
    "\n",
    "def perform_tokenization(text):\n",
    "    \"\"\"Uses NLTK's word_tokenize to split text after converting to lowercase.\"\"\"\n",
    "    return word_tokenize(str(text).lower())\n",
    "\n",
    "# Apply tokenization to the 'Description' column\n",
    "try:\n",
    "    df['tokens'] = df[TEXT_COLUMN].apply(perform_tokenization)\n",
    "except LookupError as e:\n",
    "    print(\"\\n-------------------------------------------------------------\")\n",
    "    print(\"TOKENIZATION FAILED DUE TO MISSING NLTK RESOURCE.\")\n",
    "    print(\"Please manually run the following in a Python terminal or notebook cell:\")\n",
    "    print(\">>> import nltk\")\n",
    "    print(\">>> nltk.download('punkt')\")\n",
    "    print(f\"\\nOriginal Error: {e}\")\n",
    "    print(\"-------------------------------------------------------------\")\n",
    "    # If tokenization fails, we stop the notebook here to prevent further errors.\n",
    "    # In a true notebook, you might handle this by assigning an empty list.\n",
    "    df['tokens'] = [[]] * len(df) # Fallback to prevent immediate crash, though subsequent steps will be affected.\n",
    "    \n",
    "sample_text = df[TEXT_COLUMN].iloc[0] if len(df) > 0 else \"Sample text missing\"\n",
    "sample_tokens = df['tokens'].iloc[0] if len(df) > 0 and isinstance(df['tokens'].iloc[0], list) else [\"tokens\", \"missing\"]\n",
    "\n",
    "print(f\"Original Text Sample:\\n'{sample_text}'\")\n",
    "print(f\"\\nResulting Tokens:\\n{sample_tokens}\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c040922-cbbb-475e-87ab-3a81cd856ad4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3: Word Embeddings (Conceptual Explanation and Basic Vectorization)\n",
      "\n",
      "--- Conceptual Explanation ---\n",
      "True Word Embeddings (like Word2Vec, GloVe, or BERT) require large pre-trained models,\n",
      "which cannot be loaded here. The following demonstrates the idea of turning words into vectors\n",
      "using a simple frequency method, which is the precursor to modern embeddings.\n",
      "\n",
      "--- Basic Vectorization Illustration (Frequency Encoding) ---\n",
      "Vocabulary Size (Unique words in sample data): 2449\n",
      "Word: 'hanging' -> Frequency in sample text: 1\n",
      "Vector Snippet for Sample Text (showing first 10 dimensions):\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Step 3: Word Embeddings (Conceptual Demonstration) ---\n",
    "print(\"\\nStep 3: Word Embeddings (Conceptual Explanation and Basic Vectorization)\")\n",
    "# Word Embeddings are dense, low-dimensional vector representations of words. \n",
    "# They are designed to capture semantic and syntactic relationships, meaning words with similar \n",
    "# meanings are closer together in the vector space (e.g., 'king' and 'queen').\n",
    "\n",
    "print(\"\\n--- Conceptual Explanation ---\")\n",
    "print(\"True Word Embeddings (like Word2Vec, GloVe, or BERT) require large pre-trained models,\")\n",
    "print(\"which cannot be loaded here. The following demonstrates the idea of turning words into vectors\")\n",
    "print(\"using a simple frequency method, which is the precursor to modern embeddings.\")\n",
    "\n",
    "# Create a vocabulary from all tokens\n",
    "all_tokens = [token for sublist in df['tokens'] for token in sublist]\n",
    "vocab = list(set(all_tokens))\n",
    "\n",
    "# Create a simple frequency vector for the first sample text\n",
    "word_counts = Counter(sample_tokens)\n",
    "sample_vector = [word_counts.get(word, 0) for word in vocab]\n",
    "\n",
    "print(\"\\n--- Basic Vectorization Illustration (Frequency Encoding) ---\")\n",
    "print(f\"Vocabulary Size (Unique words in sample data): {len(vocab)}\")\n",
    "print(f\"Word: '{sample_tokens[1]}' -> Frequency in sample text: {word_counts[sample_tokens[1]]}\")\n",
    "print(f\"Vector Snippet for Sample Text (showing first 10 dimensions):\\n{sample_vector[:10]}\")\n",
    "print(\"-\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223cb938-9fc3-460f-92e8-4d9f19618276",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================================================================\n",
    "# II. ASSIGNMENT 11: NLP PREPROCESSING AND TF-IDF\n",
    "# ========================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2e8da06d-26d0-4d22-b999-3e1e9dc2e548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========== PART II: ASSIGNMENT 11 - PREPROCESSING AND TF-IDF ==========\n",
      "\n",
      "Step 4: Applying Full Preprocessing Pipeline (Cleaning, Stopword Removal, Lemmatization)\n",
      "Original Text Sample:\n",
      "'WHITE HANGING HEART T-LIGHT HOLDER'\n",
      "\n",
      "Processed Text Sample (ready for TF-IDF):\n",
      "'white hanging heart tlight holder'\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Instantiate tools for preprocessing\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "ENGLISH_STOPWORDS = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text_for_tfidf(text):\n",
    "    \"\"\"\n",
    "    Applies the full preprocessing pipeline: Cleaning, Tokenization, Stopword Removal, and Lemmatization.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # --- Step 4a: Cleaning ---\n",
    "    # Removal of punctuation, numbers, and non-alphabetic characters. This ensures only words remain.\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "\n",
    "    # --- Step 4b: Tokenization and Stopword Removal ---\n",
    "    # We re-tokenize after cleaning. Stopwords (common words like 'a', 'the', 'is') are removed \n",
    "    # because they don't carry much meaning for classification/regression.\n",
    "    # We include a try-except here as well, to catch the 'punkt' error if it wasn't fixed.\n",
    "    try:\n",
    "        tokens = word_tokenize(text)\n",
    "    except LookupError:\n",
    "        tokens = text.split() # Fallback to simple split if NLTK model is missing\n",
    "\n",
    "    filtered_tokens = [w for w in tokens if w not in ENGLISH_STOPWORDS]\n",
    "\n",
    "    # --- Step 4c: Lemmatization ---\n",
    "    # Lemmatization reduces a word to its base or dictionary form (e.g., 'running' -> 'run', 'feet' -> 'foot').\n",
    "    # This helps in reducing the vocabulary size and ensures different forms of the same word are treated equally.\n",
    "    lemmas = [lemmatizer.lemmatize(w) for w in filtered_tokens]\n",
    "\n",
    "    return \" \".join(lemmas)\n",
    "\n",
    "\n",
    "# --- Step 4: Apply Full Preprocessing Pipeline ---\n",
    "print(\"\\nStep 4: Applying Full Preprocessing Pipeline (Cleaning, Stopword Removal, Lemmatization)\")\n",
    "df['processed_text'] = df[TEXT_COLUMN].apply(preprocess_text_for_tfidf)\n",
    "\n",
    "print(f\"Original Text Sample:\\n'{sample_text}'\")\n",
    "print(f\"\\nProcessed Text Sample (ready for TF-IDF):\\n'{df['processed_text'].iloc[0]}'\")\n",
    "print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d5f094c7-9af5-4a41-8cb3-22ee9a499662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 5: Applying TF-IDF Vectorization\n",
      "TF-IDF Matrix Shape: (540455, 100) (Rows: documents, Columns: features/words)\n",
      "\n",
      "First 5 rows of the resulting TF-IDF feature matrix:\n",
      "   antique  apple  assorted  bag  bird  birthday  black  blue    bottle  bowl  \\\n",
      "0      0.0    0.0       0.0  0.0   0.0       0.0    0.0   0.0  0.000000   0.0   \n",
      "1      0.0    0.0       0.0  0.0   0.0       0.0    0.0   0.0  0.000000   0.0   \n",
      "2      0.0    0.0       0.0  0.0   0.0       0.0    0.0   0.0  0.000000   0.0   \n",
      "3      0.0    0.0       0.0  0.0   0.0       0.0    0.0   0.0  0.470322   0.0   \n",
      "4      0.0    0.0       0.0  0.0   0.0       0.0    0.0   0.0  0.000000   0.0   \n",
      "\n",
      "   ...  vintage  wall     water     white  wicker  wood  wooden  woodland  \\\n",
      "0  ...      0.0   0.0  0.000000  0.444535     0.0   0.0     0.0       0.0   \n",
      "1  ...      0.0   0.0  0.000000  0.707222     0.0   0.0     0.0       0.0   \n",
      "2  ...      0.0   0.0  0.000000  0.000000     0.0   0.0     0.0       0.0   \n",
      "3  ...      0.0   0.0  0.490409  0.000000     0.0   0.0     0.0       0.0   \n",
      "4  ...      0.0   0.0  0.000000  0.650612     0.0   0.0     0.0       0.0   \n",
      "\n",
      "   wrap  zinc  \n",
      "0   0.0   0.0  \n",
      "1   0.0   0.0  \n",
      "2   0.0   0.0  \n",
      "3   0.0   0.0  \n",
      "4   0.0   0.0  \n",
      "\n",
      "[5 rows x 100 columns]\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# --- Step 5: TF-IDF Vectorization ---\n",
    "print(\"\\nStep 5: Applying TF-IDF Vectorization\")\n",
    "# TF-IDF (Term Frequency-Inverse Document Frequency) assigns a weight to each word based on two factors:\n",
    "# 1. Term Frequency (TF): How often the word appears in the current document.\n",
    "# 2. Inverse Document Frequency (IDF): How rare the word is across the entire dataset.\n",
    "# Words that are frequent in a specific description but rare overall receive a high weight.\n",
    "# This results in a matrix where each row is a document and each column is a feature (word), with values being the TF-IDF scores. \n",
    "\n",
    "# Initialize the TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "\n",
    "# Fit the vectorizer on the processed text (learning the vocabulary and calculating IDF values)\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['processed_text'])\n",
    "\n",
    "# Convert the resulting sparse matrix to a DataFrame for inspection\n",
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(f\"TF-IDF Matrix Shape: {tfidf_df.shape} (Rows: documents, Columns: features/words)\")\n",
    "print(\"\\nFirst 5 rows of the resulting TF-IDF feature matrix:\")\n",
    "print(tfidf_df.head())\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d3dfac-b5e1-48a6-acf2-0cc967d26e6d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
